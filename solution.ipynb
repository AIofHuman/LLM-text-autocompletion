{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "692db515",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f76fbf51",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0, './src')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c385aec",
   "metadata": {},
   "source": [
    "# Import libs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "01103ff6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/borisfox/Documents/ML/nn/sprint 2/project/LLM-text-autocompletion/.venv/lib/python3.13/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "import pandas as pd\n",
    "from transformers import BertTokenizerFast\n",
    "from torch.utils.data import DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "from utils import dataset_processed\n",
    "from next_token_dataset import NextTokenDataset, ValTokenDataset\n",
    "from LSTM import LSTMAutocomplete\n",
    "\n",
    "BASE_DIR = Path().resolve()\n",
    "MAX_LEN = 140\n",
    "BATCH_SIZE = 256\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ff9382f",
   "metadata": {},
   "source": [
    "# 1. Clean raw data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1c484c5d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>switchfoot awww thats a bummer you shoulda got...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>is upset that he cant update his facebook by t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>kenichan i dived many times for the ball manag...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>my whole body feels itchy and like its on fire</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>nationwideclass no its not behaving at all im ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text\n",
       "0  switchfoot awww thats a bummer you shoulda got...\n",
       "1  is upset that he cant update his facebook by t...\n",
       "2  kenichan i dived many times for the ball manag...\n",
       "3     my whole body feels itchy and like its on fire\n",
       "4  nationwideclass no its not behaving at all im ..."
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_processed(\n",
    "    os.path.join(BASE_DIR, 'data', 'tweets.txt'),\n",
    "    os.path.join(BASE_DIR, 'data', 'cleaned_tweets.csv')\n",
    ")\n",
    "dataset = pd.read_csv(os.path.join(BASE_DIR, 'data', 'cleaned_tweets.csv'), index_col=False)\n",
    "dataset.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "879b6e8f",
   "metadata": {},
   "source": [
    "# 2. Split dataset by train, val, test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "107242ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train texts: 1280398, Val texts: 160050, Test texts: 160050\n"
     ]
    }
   ],
   "source": [
    "train, val = train_test_split(dataset, test_size=0.2, random_state=42)\n",
    "val, test  = train_test_split(val, test_size=0.5, random_state=42)\n",
    "print(f\"Train texts: {len(train)}, Val texts: {len(val)}, Test texts: {len(test)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "901e912e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for limit of calc resources make val and test selection shoter\n",
    "val = val.sample(n=100, random_state=42)\n",
    "test = test.sample(n=100, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "187aed36",
   "metadata": {},
   "source": [
    "# 3. Create datasets and data loader "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a082a529",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizerFast.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "\n",
    "train_dataset = NextTokenDataset(train, tokenizer, seq_length=MAX_LEN)\n",
    "val_dataset = ValTokenDataset(val, tokenizer, seq_length=MAX_LEN)\n",
    "test_dataset = ValTokenDataset(test, tokenizer, seq_length=MAX_LEN)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE)\n",
    "val_loader = DataLoader(val_dataset, batch_size=1)\n",
    "test_loader = DataLoader(test_dataset, batch_size=1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e847faba",
   "metadata": {},
   "source": [
    "# 4. Train LSTM model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7870f50",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 2, 3, 4]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mnotebook controller is DISPOSED. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "test_list = [1,2,3,4,5,6,7]\n",
    "test_list[:-3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2276b851",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training on cpu\n",
      "Training samples: 3\n",
      "Validation samples: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TOKENIZERS_PARALLELISM=(true | false)\n",
      "Epoch 1/10 [Train]: 100%|██████████| 1/1 [00:00<00:00,  4.62it/s, Loss=10.3465]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10:\n",
      "  Train Loss: 10.3465\n",
      "  Val loss: 10.3241, rouge-1: 0.0000, val rouge2: 0.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/10 [Train]: 100%|██████████| 1/1 [00:00<00:00,  9.28it/s, Loss=10.3675]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/10:\n",
      "  Train Loss: 10.3675\n",
      "  Val loss: 10.3220, rouge-1: 0.0000, val rouge2: 0.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/10 [Train]: 100%|██████████| 1/1 [00:00<00:00,  9.56it/s, Loss=10.2899]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/10:\n",
      "  Train Loss: 10.2899\n",
      "  Val loss: 10.3194, rouge-1: 0.0000, val rouge2: 0.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/10 [Train]: 100%|██████████| 1/1 [00:00<00:00,  9.06it/s, Loss=10.2950]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/10:\n",
      "  Train Loss: 10.2950\n",
      "  Val loss: 10.3167, rouge-1: 0.0000, val rouge2: 0.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/10 [Train]: 100%|██████████| 1/1 [00:00<00:00,  9.78it/s, Loss=10.2050]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/10:\n",
      "  Train Loss: 10.2050\n",
      "  Val loss: 10.3136, rouge-1: 0.0000, val rouge2: 0.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6/10 [Train]: 100%|██████████| 1/1 [00:00<00:00,  9.37it/s, Loss=10.1418]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/10:\n",
      "  Train Loss: 10.1418\n",
      "  Val loss: 10.3103, rouge-1: 0.0000, val rouge2: 0.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7/10 [Train]: 100%|██████████| 1/1 [00:00<00:00,  9.85it/s, Loss=10.0311]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7/10:\n",
      "  Train Loss: 10.0311\n",
      "  Val loss: 10.3066, rouge-1: 0.0000, val rouge2: 0.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8/10 [Train]: 100%|██████████| 1/1 [00:00<00:00,  9.81it/s, Loss=9.9809]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8/10:\n",
      "  Train Loss: 9.9809\n",
      "  Val loss: 10.3026, rouge-1: 0.0000, val rouge2: 0.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9/10 [Train]: 100%|██████████| 1/1 [00:00<00:00,  9.09it/s, Loss=9.8839]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9/10:\n",
      "  Train Loss: 9.8839\n",
      "  Val loss: 10.2984, rouge-1: 0.0000, val rouge2: 0.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10/10 [Train]: 100%|██████████| 1/1 [00:00<00:00,  6.87it/s, Loss=9.8156]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/10:\n",
      "  Train Loss: 9.8156\n",
      "  Val loss: 10.2937, rouge-1: 0.0000, val rouge2: 0.0000\n"
     ]
    }
   ],
   "source": [
    "from train_LSTM import train_model as train_LSTM_model\n",
    "model = LSTMAutocomplete(tokenizer.vocab_size)\n",
    "train_LSTM_model(model, train_loader, val_loader, tokenizer.vocab, device='cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4d12d29",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mnotebook controller is DISPOSED. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mnotebook controller is DISPOSED. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mnotebook controller is DISPOSED. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0ff4721",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mnotebook controller is DISPOSED. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mnotebook controller is DISPOSED. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mnotebook controller is DISPOSED. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
