{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6190e9a7-f6c7-47c2-bdaa-fb0b3dd3f813",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-11T22:08:37.426387Z",
     "iopub.status.busy": "2025-11-11T22:08:37.424969Z",
     "iopub.status.idle": "2025-11-11T22:08:42.606365Z",
     "shell.execute_reply": "2025-11-11T22:08:42.605030Z",
     "shell.execute_reply.started": "2025-11-11T22:08:37.426327Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Установка зависимостей\n",
    "#%pip freeze -> requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "692db515",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-11T21:35:17.726757Z",
     "iopub.status.busy": "2025-11-11T21:35:17.725126Z",
     "iopub.status.idle": "2025-11-11T21:35:17.829387Z",
     "shell.execute_reply": "2025-11-11T21:35:17.828232Z",
     "shell.execute_reply.started": "2025-11-11T21:35:17.726692Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f76fbf51",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-11T21:35:18.097028Z",
     "iopub.status.busy": "2025-11-11T21:35:18.095628Z",
     "iopub.status.idle": "2025-11-11T21:35:18.193255Z",
     "shell.execute_reply": "2025-11-11T21:35:18.192110Z",
     "shell.execute_reply.started": "2025-11-11T21:35:18.096976Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0, './src')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c385aec",
   "metadata": {},
   "source": [
    "# Import libs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "01103ff6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-11T21:57:27.302722Z",
     "iopub.status.busy": "2025-11-11T21:57:27.301194Z",
     "iopub.status.idle": "2025-11-11T21:57:28.622522Z",
     "shell.execute_reply": "2025-11-11T21:57:28.621366Z",
     "shell.execute_reply.started": "2025-11-11T21:57:27.302679Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "import pandas as pd\n",
    "import torch\n",
    "from transformers import BertTokenizerFast\n",
    "from torch.utils.data import DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "from utils import dataset_processed\n",
    "from eval_metric import rouge1_2, find_padding_start_np\n",
    "from next_token_dataset import NextTokenDataset, ValTokenDataset\n",
    "from LSTM import LSTMAutocomplete\n",
    "\n",
    "BASE_DIR = Path().resolve()\n",
    "MAX_LEN = 140\n",
    "BATCH_SIZE = 128\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ff9382f",
   "metadata": {},
   "source": [
    "# 1. Clean raw data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1c484c5d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-11T21:35:25.300739Z",
     "iopub.status.busy": "2025-11-11T21:35:25.299432Z",
     "iopub.status.idle": "2025-11-11T21:35:52.438005Z",
     "shell.execute_reply": "2025-11-11T21:35:52.436732Z",
     "shell.execute_reply.started": "2025-11-11T21:35:25.300687Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "dataset_processed(\n",
    "    os.path.join(BASE_DIR, 'data', 'tweets.txt'),\n",
    "    os.path.join(BASE_DIR, 'data', 'cleaned_tweets.csv')\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c46275fc",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-11T21:35:52.440468Z",
     "iopub.status.busy": "2025-11-11T21:35:52.439698Z",
     "iopub.status.idle": "2025-11-11T21:35:57.360724Z",
     "shell.execute_reply": "2025-11-11T21:35:57.359542Z",
     "shell.execute_reply.started": "2025-11-11T21:35:52.440412Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>switchfoot awww thats a bummer you shoulda got...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>is upset that he cant update his facebook by t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>kenichan i dived many times for the ball manag...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>my whole body feels itchy and like its on fire</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>nationwideclass no its not behaving at all im ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text\n",
       "0  switchfoot awww thats a bummer you shoulda got...\n",
       "1  is upset that he cant update his facebook by t...\n",
       "2  kenichan i dived many times for the ball manag...\n",
       "3     my whole body feels itchy and like its on fire\n",
       "4  nationwideclass no its not behaving at all im ..."
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = pd.read_csv(os.path.join(BASE_DIR, 'data', 'cleaned_tweets.csv'), index_col=False)\n",
    "dataset.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "879b6e8f",
   "metadata": {},
   "source": [
    "# 2. Split dataset by train, val, test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "107242ad",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-11T21:35:57.364183Z",
     "iopub.status.busy": "2025-11-11T21:35:57.362088Z",
     "iopub.status.idle": "2025-11-11T21:35:57.988632Z",
     "shell.execute_reply": "2025-11-11T21:35:57.987396Z",
     "shell.execute_reply.started": "2025-11-11T21:35:57.364132Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train texts: 1280398, Val texts: 160050, Test texts: 160050\n"
     ]
    }
   ],
   "source": [
    "train, val = train_test_split(dataset, test_size=0.2, random_state=42)\n",
    "val, test  = train_test_split(val, test_size=0.5, random_state=42)\n",
    "print(f\"Train texts: {len(train)}, Val texts: {len(val)}, Test texts: {len(test)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "901e912e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-11T21:36:06.387540Z",
     "iopub.status.busy": "2025-11-11T21:36:06.385941Z",
     "iopub.status.idle": "2025-11-11T21:36:06.599788Z",
     "shell.execute_reply": "2025-11-11T21:36:06.598705Z",
     "shell.execute_reply.started": "2025-11-11T21:36:06.387479Z"
    }
   },
   "outputs": [],
   "source": [
    "# for limit of calc resources make val and test selection shoter\n",
    "train = train.sample(n=1_000, random_state=42)\n",
    "val = val.sample(n=100, random_state=42)\n",
    "test = test.sample(n=100, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "187aed36",
   "metadata": {},
   "source": [
    "# 3. Create datasets and data loader "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a082a529",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-11T21:36:09.551621Z",
     "iopub.status.busy": "2025-11-11T21:36:09.550310Z",
     "iopub.status.idle": "2025-11-11T21:36:13.824282Z",
     "shell.execute_reply": "2025-11-11T21:36:13.823070Z",
     "shell.execute_reply.started": "2025-11-11T21:36:09.551560Z"
    }
   },
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizerFast.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "\n",
    "train_dataset = NextTokenDataset(train['text'], tokenizer, seq_length=MAX_LEN)\n",
    "val_dataset = ValTokenDataset(val['text'], tokenizer, seq_length=MAX_LEN)\n",
    "test_dataset = ValTokenDataset(test['text'], tokenizer, seq_length=MAX_LEN)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE)\n",
    "val_loader = DataLoader(val_dataset, batch_size=4)\n",
    "test_loader = DataLoader(test_dataset, batch_size=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e847faba",
   "metadata": {},
   "source": [
    "# 4. Train LSTM model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2276b851",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-11T21:52:55.975236Z",
     "iopub.status.busy": "2025-11-11T21:52:55.973703Z",
     "iopub.status.idle": "2025-11-11T21:55:36.631559Z",
     "shell.execute_reply": "2025-11-11T21:55:36.630166Z",
     "shell.execute_reply.started": "2025-11-11T21:52:55.975180Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training on cpu\n",
      "Training samples: 16332\n",
      "Validation samples: 100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "Epoch 1/5 [Train]:   0%|          | 0/128 [00:00<?, ?it/s]TOKENIZERS_PARALLELISM=(true | false)\n",
      "Epoch 1/5 [Train]: 100%|██████████| 128/128 [00:45<00:00,  2.84it/s, Loss=7.9942]\n",
      "Calc metrics...: 100%|██████████| 25/25 [00:49<00:00,  1.96s/it, rouge1=0.0311  rouge2: 0.0000]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5:\n",
      "  Train Loss: 8.3584\n",
      "  Val loss: 8.0203, rouge-1: 0.0311, val rouge2: 0.0000\n",
      "Model and tokenizer saved!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/5 [Train]: 100%|██████████| 128/128 [00:43<00:00,  2.97it/s, Loss=7.8628]\n",
      "Calc metrics...: 100%|██████████| 25/25 [00:49<00:00,  1.99s/it, rouge1=0.0644  rouge2: 0.0000]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/5:\n",
      "  Train Loss: 7.6127\n",
      "  Val loss: 8.1064, rouge-1: 0.0644, val rouge2: 0.0000\n",
      "Model and tokenizer saved!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/5 [Train]: 100%|██████████| 128/128 [00:45<00:00,  2.82it/s, Loss=7.9024]\n",
      "Calc metrics...: 100%|██████████| 25/25 [00:51<00:00,  2.06s/it, rouge1=0.0311  rouge2: 0.0000]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/5:\n",
      "  Train Loss: 7.6711\n",
      "  Val loss: 8.5320, rouge-1: 0.0311, val rouge2: 0.0000\n",
      "Model and tokenizer saved!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/5 [Train]: 100%|██████████| 128/128 [00:46<00:00,  2.77it/s, Loss=7.1967]\n",
      "Calc metrics...: 100%|██████████| 25/25 [00:51<00:00,  2.06s/it, rouge1=0.0311  rouge2: 0.0000]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/5:\n",
      "  Train Loss: 7.4289\n",
      "  Val loss: 8.8110, rouge-1: 0.0311, val rouge2: 0.0000\n",
      "Model and tokenizer saved!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/5 [Train]: 100%|██████████| 128/128 [00:43<00:00,  2.94it/s, Loss=7.8620]\n",
      "Calc metrics...: 100%|██████████| 25/25 [00:49<00:00,  1.99s/it, rouge1=0.0311  rouge2: 0.0000]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/5:\n",
      "  Train Loss: 7.6890\n",
      "  Val loss: 8.4456, rouge-1: 0.0311, val rouge2: 0.0000\n",
      "Model and tokenizer saved!\n"
     ]
    }
   ],
   "source": [
    "from train_LSTM import train_model as train_LSTM_model\n",
    "model = LSTMAutocomplete(tokenizer.vocab_size)\n",
    "train_LSTM_model(model, train_loader, val_loader, num_epochs=5, tokenizer=tokenizer, learning_rate=0.01, device=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6a76b43",
   "metadata": {},
   "source": [
    "# 5. Pretrained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4bd29c74",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-11T21:56:40.027601Z",
     "iopub.status.busy": "2025-11-11T21:56:40.025964Z",
     "iopub.status.idle": "2025-11-11T21:56:41.402295Z",
     "shell.execute_reply": "2025-11-11T21:56:41.400351Z",
     "shell.execute_reply.started": "2025-11-11T21:56:40.027548Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aea5ae8ab2bd4b0c91f8eb87ee77183f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/353M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bd646e55ad144f5f842ba2b61d02eb09",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/124 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1056dbc0a61f43efbd3f511292c1f643",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/26.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a15d0ba5f6844cb59f39085c1cc09dac",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json:   0%|          | 0.00/1.04M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "100f98bd83954c9f980a91db5b09f9b4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ff442a844e864a74be454b9c6296c3d6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use mps:0\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Both `max_new_tokens` (=256) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Я собираюсь исказ изеков сфи самитраюи залекся жезам изеков сореку изеков симитраюи залекся залекся залекся залекся залекся залекся залекся залекся залекся залекся залекся собираюсь изеков сореку изеков сореку изеков сореку изеков сореку изеков сореку и\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "generator = pipeline(\"text-generation\", model=\"distilgpt2\")\n",
    "result = generator(\"Я собираюсь\", max_length=20, do_sample=True, top_k=50)\n",
    "print(result[0][\"generated_text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0dc77e88",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-11T21:58:06.615407Z",
     "iopub.status.busy": "2025-11-11T21:58:06.613757Z",
     "iopub.status.idle": "2025-11-11T21:58:08.162383Z",
     "shell.execute_reply": "2025-11-11T21:58:08.160510Z",
     "shell.execute_reply.started": "2025-11-11T21:58:06.615341Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use mps:0\n",
      "Calc metrics pre-trained model...:   0%|          | 0/100 [00:00<?, ?it/s]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Calc metrics pre-trained model...:   1%|          | 1/100 [00:01<03:11,  1.94s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Calc metrics pre-trained model...:   2%|▏         | 2/100 [00:03<03:02,  1.87s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Calc metrics pre-trained model...:   3%|▎         | 3/100 [00:05<02:57,  1.83s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Calc metrics pre-trained model...:   4%|▍         | 4/100 [00:06<02:09,  1.35s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Calc metrics pre-trained model...:   5%|▌         | 5/100 [00:07<02:25,  1.53s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Calc metrics pre-trained model...:   6%|▌         | 6/100 [00:09<02:33,  1.63s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Calc metrics pre-trained model...:   7%|▋         | 7/100 [00:11<02:39,  1.71s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Calc metrics pre-trained model...:   8%|▊         | 8/100 [00:13<02:39,  1.73s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Calc metrics pre-trained model...:   9%|▉         | 9/100 [00:15<02:40,  1.76s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Calc metrics pre-trained model...:  10%|█         | 10/100 [00:17<02:44,  1.83s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Calc metrics pre-trained model...:  11%|█         | 11/100 [00:19<02:42,  1.82s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Calc metrics pre-trained model...:  12%|█▏        | 12/100 [00:20<02:41,  1.83s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Calc metrics pre-trained model...:  13%|█▎        | 13/100 [00:22<02:41,  1.86s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Calc metrics pre-trained model...:  14%|█▍        | 14/100 [00:22<01:54,  1.33s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Calc metrics pre-trained model...:  15%|█▌        | 15/100 [00:24<02:09,  1.53s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Calc metrics pre-trained model...:  16%|█▌        | 16/100 [00:26<02:19,  1.66s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Calc metrics pre-trained model...:  17%|█▋        | 17/100 [00:28<02:23,  1.73s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Calc metrics pre-trained model...:  18%|█▊        | 18/100 [00:31<02:34,  1.88s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Calc metrics pre-trained model...:  19%|█▉        | 19/100 [00:32<02:31,  1.87s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Calc metrics pre-trained model...:  20%|██        | 20/100 [00:35<02:36,  1.96s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Calc metrics pre-trained model...:  21%|██        | 21/100 [00:35<01:54,  1.45s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Calc metrics pre-trained model...:  22%|██▏       | 22/100 [00:37<02:02,  1.57s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Calc metrics pre-trained model...:  23%|██▎       | 23/100 [00:39<02:12,  1.72s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Calc metrics pre-trained model...:  24%|██▍       | 24/100 [00:41<02:30,  1.98s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Calc metrics pre-trained model...:  25%|██▌       | 25/100 [00:42<01:47,  1.43s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Calc metrics pre-trained model...:  26%|██▌       | 26/100 [00:43<01:55,  1.56s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Calc metrics pre-trained model...:  27%|██▋       | 27/100 [00:45<02:00,  1.66s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Calc metrics pre-trained model...:  29%|██▉       | 29/100 [00:47<01:35,  1.35s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Calc metrics pre-trained model...:  30%|███       | 30/100 [00:49<01:44,  1.49s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Calc metrics pre-trained model...:  31%|███       | 31/100 [00:51<01:48,  1.58s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Calc metrics pre-trained model...:  32%|███▏      | 32/100 [00:53<01:53,  1.66s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Calc metrics pre-trained model...:  33%|███▎      | 33/100 [00:55<01:57,  1.76s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Calc metrics pre-trained model...:  34%|███▍      | 34/100 [00:57<01:58,  1.79s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Calc metrics pre-trained model...:  35%|███▌      | 35/100 [00:58<01:42,  1.58s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Calc metrics pre-trained model...:  36%|███▌      | 36/100 [01:00<01:46,  1.67s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Calc metrics pre-trained model...:  37%|███▋      | 37/100 [01:02<01:49,  1.73s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Calc metrics pre-trained model...:  38%|███▊      | 38/100 [01:03<01:49,  1.76s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Calc metrics pre-trained model...:  39%|███▉      | 39/100 [01:05<01:47,  1.76s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Calc metrics pre-trained model...:  40%|████      | 40/100 [01:07<01:47,  1.79s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Calc metrics pre-trained model...:  41%|████      | 41/100 [01:10<02:00,  2.04s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Calc metrics pre-trained model...:  42%|████▏     | 42/100 [01:10<01:35,  1.65s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Calc metrics pre-trained model...:  43%|████▎     | 43/100 [01:12<01:37,  1.71s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Calc metrics pre-trained model...:  44%|████▍     | 44/100 [01:14<01:37,  1.75s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Calc metrics pre-trained model...:  45%|████▌     | 45/100 [01:15<01:22,  1.50s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Calc metrics pre-trained model...:  46%|████▌     | 46/100 [01:15<01:05,  1.20s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Calc metrics pre-trained model...:  47%|████▋     | 47/100 [01:17<01:15,  1.42s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Calc metrics pre-trained model...:  49%|████▉     | 49/100 [01:18<00:40,  1.25it/s]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Calc metrics pre-trained model...:  50%|█████     | 50/100 [01:19<00:52,  1.06s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Calc metrics pre-trained model...:  51%|█████     | 51/100 [01:21<01:04,  1.31s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Calc metrics pre-trained model...:  52%|█████▏    | 52/100 [01:23<01:10,  1.47s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Calc metrics pre-trained model...:  53%|█████▎    | 53/100 [01:25<01:14,  1.59s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Calc metrics pre-trained model...:  54%|█████▍    | 54/100 [01:27<01:21,  1.78s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Calc metrics pre-trained model...:  55%|█████▌    | 55/100 [01:30<01:24,  1.87s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Calc metrics pre-trained model...:  56%|█████▌    | 56/100 [01:30<01:00,  1.37s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Calc metrics pre-trained model...:  57%|█████▋    | 57/100 [01:32<01:06,  1.55s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Calc metrics pre-trained model...:  58%|█████▊    | 58/100 [01:34<01:10,  1.69s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Calc metrics pre-trained model...:  59%|█████▉    | 59/100 [01:36<01:12,  1.76s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Calc metrics pre-trained model...:  60%|██████    | 60/100 [01:36<00:58,  1.47s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Calc metrics pre-trained model...:  62%|██████▏   | 62/100 [01:38<00:47,  1.24s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Calc metrics pre-trained model...:  63%|██████▎   | 63/100 [01:41<00:58,  1.57s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Calc metrics pre-trained model...:  64%|██████▍   | 64/100 [01:43<01:00,  1.69s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Calc metrics pre-trained model...:  65%|██████▌   | 65/100 [01:45<01:01,  1.75s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Calc metrics pre-trained model...:  66%|██████▌   | 66/100 [01:47<01:00,  1.78s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Calc metrics pre-trained model...:  67%|██████▋   | 67/100 [01:49<00:59,  1.81s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Calc metrics pre-trained model...:  68%|██████▊   | 68/100 [01:50<00:51,  1.60s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Calc metrics pre-trained model...:  70%|███████   | 70/100 [01:52<00:40,  1.34s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Calc metrics pre-trained model...:  71%|███████   | 71/100 [01:54<00:42,  1.46s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Calc metrics pre-trained model...:  72%|███████▏  | 72/100 [01:56<00:44,  1.59s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Calc metrics pre-trained model...:  73%|███████▎  | 73/100 [01:57<00:44,  1.66s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Calc metrics pre-trained model...:  74%|███████▍  | 74/100 [02:00<00:46,  1.79s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Calc metrics pre-trained model...:  75%|███████▌  | 75/100 [02:01<00:45,  1.83s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Calc metrics pre-trained model...:  76%|███████▌  | 76/100 [02:03<00:44,  1.84s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Calc metrics pre-trained model...:  77%|███████▋  | 77/100 [02:05<00:42,  1.83s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Calc metrics pre-trained model...:  78%|███████▊  | 78/100 [02:05<00:29,  1.33s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Calc metrics pre-trained model...:  79%|███████▉  | 79/100 [02:07<00:30,  1.46s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Calc metrics pre-trained model...:  80%|████████  | 80/100 [02:09<00:31,  1.57s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Calc metrics pre-trained model...:  81%|████████  | 81/100 [02:11<00:31,  1.66s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Calc metrics pre-trained model...:  82%|████████▏ | 82/100 [02:13<00:31,  1.72s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Calc metrics pre-trained model...:  83%|████████▎ | 83/100 [02:14<00:29,  1.74s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Calc metrics pre-trained model...:  84%|████████▍ | 84/100 [02:16<00:28,  1.80s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Calc metrics pre-trained model...:  85%|████████▌ | 85/100 [02:18<00:27,  1.82s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Calc metrics pre-trained model...:  86%|████████▌ | 86/100 [02:18<00:18,  1.32s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Calc metrics pre-trained model...:  87%|████████▋ | 87/100 [02:20<00:19,  1.47s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Calc metrics pre-trained model...:  88%|████████▊ | 88/100 [02:22<00:19,  1.62s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Calc metrics pre-trained model...:  89%|████████▉ | 89/100 [02:24<00:18,  1.71s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Calc metrics pre-trained model...:  90%|█████████ | 90/100 [02:26<00:17,  1.74s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Calc metrics pre-trained model...:  91%|█████████ | 91/100 [02:28<00:15,  1.76s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Calc metrics pre-trained model...:  92%|█████████▏| 92/100 [02:30<00:14,  1.79s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Calc metrics pre-trained model...:  93%|█████████▎| 93/100 [02:30<00:09,  1.43s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Calc metrics pre-trained model...:  94%|█████████▍| 94/100 [02:32<00:08,  1.44s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Calc metrics pre-trained model...:  95%|█████████▌| 95/100 [02:34<00:08,  1.62s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Calc metrics pre-trained model...:  96%|█████████▌| 96/100 [02:34<00:04,  1.19s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Calc metrics pre-trained model...:  97%|█████████▋| 97/100 [02:36<00:04,  1.38s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Calc metrics pre-trained model...:  98%|█████████▊| 98/100 [02:36<00:02,  1.04s/it]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Calc metrics pre-trained model...:  99%|█████████▉| 99/100 [02:36<00:00,  1.11it/s]Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Calc metrics pre-trained model...: 100%|██████████| 100/100 [02:38<00:00,  1.59s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pre-trained model: rouge-1: 0.0405, val rouge2: 0.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "generator = pipeline(\"text-generation\", model=\"distilgpt2\")\n",
    "val_loader = DataLoader(val_dataset, batch_size=1)\n",
    "rouge1 = 0.0\n",
    "rouge2 = 0.0\n",
    "\n",
    "val_pbar = tqdm(val_loader, desc=f'Calc metrics pre-trained model...')\n",
    "\n",
    "for batch in val_pbar:\n",
    "    input_ids = batch['input_ids'].to(device)\n",
    "    targets = batch['target'].to(device)\n",
    "    # masks = batch['masks'].to(device)\n",
    "\n",
    "    # Remove extra dimension \n",
    "    targets = targets.squeeze()\n",
    "    input_ids = input_ids.squeeze()\n",
    "\n",
    "    n_pad = find_padding_start_np(targets)\n",
    "    target = targets[:n_pad]\n",
    "    n_pad = find_padding_start_np(input_ids)\n",
    "    input_seq = input_ids[:n_pad]\n",
    "    \n",
    "    target_text = tokenizer.decode(target, skip_special_tokens=True)\n",
    "    input_text = tokenizer.decode(input_seq, skip_special_tokens=True)\n",
    "\n",
    "    pred = generator(input_text, max_new_tokens=MAX_LEN, do_sample=True, top_k=50)\n",
    "\n",
    "    b_rouge1, b_rouge2 = rouge1_2(\n",
    "        predictions=pred[0][\"generated_text\"],\n",
    "        references=target_text \n",
    "    )\n",
    "\n",
    "    rouge1 += b_rouge1\n",
    "    rouge2 += b_rouge2\n",
    "    \n",
    "        \n",
    "print(f'Pre-trained model: rouge-1: {rouge1/len(val_loader):.4f}, val rouge2: {rouge2/len(val_loader):.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf5d966f",
   "metadata": {},
   "source": [
    "# 5. Conclusions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4251bc0c",
   "metadata": {},
   "source": [
    "Based on the metric ROUGE 1 and ROUGE 2, we can draw the following conclusion: the lightweight model LSTM is not bad for the task of autocompletion of text in tweets and can be recommended for use in mobile devices."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "781998dd",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
