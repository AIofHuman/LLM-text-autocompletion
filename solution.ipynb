{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "692db515",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f76fbf51",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0, './src')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c385aec",
   "metadata": {},
   "source": [
    "# Import libs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "01103ff6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "import pandas as pd\n",
    "from transformers import BertTokenizerFast\n",
    "from torch.utils.data import DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "from utils import dataset_processed\n",
    "from next_token_dataset import NextTokenDataset, ValTokenDataset\n",
    "from LSTM import LSTMAutocomplete\n",
    "\n",
    "BASE_DIR = Path().resolve()\n",
    "MAX_LEN = 140\n",
    "BATCH_SIZE = 128\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ff9382f",
   "metadata": {},
   "source": [
    "# 1. Clean raw data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1c484c5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_processed(\n",
    "    os.path.join(BASE_DIR, 'data', 'tweets.txt'),\n",
    "    os.path.join(BASE_DIR, 'data', 'cleaned_tweets.csv')\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c46275fc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>switchfoot awww thats a bummer you shoulda got...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>is upset that he cant update his facebook by t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>kenichan i dived many times for the ball manag...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>my whole body feels itchy and like its on fire</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>nationwideclass no its not behaving at all im ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text\n",
       "0  switchfoot awww thats a bummer you shoulda got...\n",
       "1  is upset that he cant update his facebook by t...\n",
       "2  kenichan i dived many times for the ball manag...\n",
       "3     my whole body feels itchy and like its on fire\n",
       "4  nationwideclass no its not behaving at all im ..."
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = pd.read_csv(os.path.join(BASE_DIR, 'data', 'cleaned_tweets.csv'), index_col=False)\n",
    "dataset.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "879b6e8f",
   "metadata": {},
   "source": [
    "# 2. Split dataset by train, val, test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "107242ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train texts: 1280398, Val texts: 160050, Test texts: 160050\n"
     ]
    }
   ],
   "source": [
    "train, val = train_test_split(dataset, test_size=0.2, random_state=42)\n",
    "val, test  = train_test_split(val, test_size=0.5, random_state=42)\n",
    "print(f\"Train texts: {len(train)}, Val texts: {len(val)}, Test texts: {len(test)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "901e912e",
   "metadata": {},
   "source": [
    "# for limit of calc resources make val and test selection shoter\n",
    "val = val.sample(n=100, random_state=42)\n",
    "test = test.sample(n=100, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "187aed36",
   "metadata": {},
   "source": [
    "# 3. Create datasets and data loader "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a082a529",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizerFast.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "\n",
    "train_dataset = NextTokenDataset(train['text'], tokenizer, seq_length=MAX_LEN)\n",
    "val_dataset = ValTokenDataset(val['text'], tokenizer, seq_length=MAX_LEN)\n",
    "test_dataset = ValTokenDataset(test['text'], tokenizer, seq_length=MAX_LEN)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE)\n",
    "val_loader = DataLoader(val_dataset, batch_size=4)\n",
    "test_loader = DataLoader(test_dataset, batch_size=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e847faba",
   "metadata": {},
   "source": [
    "# 4. Train LSTM model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2276b851",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training on cpu\n",
      "Training samples: 1\n",
      "Validation samples: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Epoch 1/10 [Train]: 100%|██████████| 1/1 [00:00<00:00,  4.27it/s, Loss=10.3263]\n",
      "Calc metrics...: 100%|██████████| 1/1 [00:01<00:00,  1.39s/it, rouge1=0.0000  rouge2: 0.0000]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10:\n",
      "  Train Loss: 10.3263\n",
      "  Val loss: 10.3256, rouge-1: 0.0000, val rouge2: 0.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/10 [Train]: 100%|██████████| 1/1 [00:00<00:00,  9.18it/s, Loss=10.2980]\n",
      "Calc metrics...: 100%|██████████| 1/1 [00:01<00:00,  1.79s/it, rouge1=0.0000  rouge2: 0.0000]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/10:\n",
      "  Train Loss: 10.2980\n",
      "  Val loss: 10.3250, rouge-1: 0.0000, val rouge2: 0.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/10 [Train]: 100%|██████████| 1/1 [00:00<00:00,  5.37it/s, Loss=10.2245]\n",
      "Calc metrics...: 100%|██████████| 1/1 [00:01<00:00,  1.59s/it, rouge1=0.0000  rouge2: 0.0000]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/10:\n",
      "  Train Loss: 10.2245\n",
      "  Val loss: 10.3238, rouge-1: 0.0000, val rouge2: 0.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/10 [Train]: 100%|██████████| 1/1 [00:00<00:00,  8.61it/s, Loss=10.0536]\n",
      "Calc metrics...:   0%|          | 0/1 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtrain_LSTM\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m train_model \u001b[38;5;28;01mas\u001b[39;00m train_LSTM_model\n\u001b[32m      2\u001b[39m model = LSTMAutocomplete(tokenizer.vocab_size)\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m \u001b[43mtrain_LSTM_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlearning_rate\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m0.01\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mcpu\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/ML/nn/sprint 2/project/LLM-text-autocompletion/src/train_LSTM.py:72\u001b[39m, in \u001b[36mtrain_model\u001b[39m\u001b[34m(model, train_loader, val_loader, tokenizer, device, num_epochs, learning_rate, save_dir)\u001b[39m\n\u001b[32m     70\u001b[39m \u001b[38;5;66;03m# Calculate training metrics\u001b[39;00m\n\u001b[32m     71\u001b[39m avg_train_loss = epoch_train_loss / \u001b[38;5;28mlen\u001b[39m(train_loader)\n\u001b[32m---> \u001b[39m\u001b[32m72\u001b[39m metrics = \u001b[43mcalc_metrics\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     74\u001b[39m \u001b[38;5;66;03m# Update learning rate\u001b[39;00m\n\u001b[32m     75\u001b[39m scheduler.step(avg_train_loss)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/ML/nn/sprint 2/project/LLM-text-autocompletion/src/eval_metric.py:87\u001b[39m, in \u001b[36mcalc_metrics\u001b[39m\u001b[34m(model, loader, criterion, tokenizer, device)\u001b[39m\n\u001b[32m     80\u001b[39m \u001b[38;5;66;03m# Remove extra dimension \u001b[39;00m\n\u001b[32m     81\u001b[39m \u001b[38;5;66;03m# targets = targets.squeeze()\u001b[39;00m\n\u001b[32m     82\u001b[39m \u001b[38;5;66;03m# masks = masks.squeeze()\u001b[39;00m\n\u001b[32m     83\u001b[39m \u001b[38;5;66;03m# predict = predicts.squeeze()\u001b[39;00m\n\u001b[32m     85\u001b[39m b_loss = criterion(predicts, targets[:,\u001b[32m0\u001b[39m])\n\u001b[32m---> \u001b[39m\u001b[32m87\u001b[39m pred_idxs = \u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mgenerate_completion\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_target_length\u001b[49m\u001b[43m=\u001b[49m\u001b[43mMAX_LEN\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     89\u001b[39m b_rouge1 = \u001b[32m0\u001b[39m\n\u001b[32m     90\u001b[39m b_rouge2 = \u001b[32m0\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/ML/nn/sprint 2/project/LLM-text-autocompletion/src/LSTM.py:98\u001b[39m, in \u001b[36mLSTMAutocomplete.generate_completion\u001b[39m\u001b[34m(self, tokenizer, input_sequence, max_target_length)\u001b[39m\n\u001b[32m     96\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m torch.no_grad():\n\u001b[32m     97\u001b[39m     \u001b[38;5;28;01mwhile\u001b[39;00m i < max_target_length:           \n\u001b[32m---> \u001b[39m\u001b[32m98\u001b[39m         output, _ = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_sequence\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     99\u001b[39m         pred = torch.argmax(output, dim=-\u001b[32m1\u001b[39m)\n\u001b[32m    100\u001b[39m         \u001b[38;5;66;03m# pred_token = tokenizer.decode(pred, skip_special_tokens=True) #tokenizer.convert_ids_to_tokens(pred)\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/ML/nn/sprint 2/project/LLM-text-autocompletion/src/LSTM.py:63\u001b[39m, in \u001b[36mLSTMAutocomplete.forward\u001b[39m\u001b[34m(self, x, attention_mask, hidden)\u001b[39m\n\u001b[32m     60\u001b[39m embedded = \u001b[38;5;28mself\u001b[39m.embedding(x)  \u001b[38;5;66;03m# (batch_size, seq_length, embedding_dim)\u001b[39;00m\n\u001b[32m     62\u001b[39m \u001b[38;5;66;03m# LSTM\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m63\u001b[39m lstm_out, hidden = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mlstm\u001b[49m\u001b[43m(\u001b[49m\u001b[43membedded\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhidden\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# lstm_out: (batch_size, seq_length, hidden_dim)\u001b[39;00m\n\u001b[32m     65\u001b[39m \u001b[38;5;66;03m# Use attention mask to find last non-padded token\u001b[39;00m\n\u001b[32m     66\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m attention_mask \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m     67\u001b[39m     \u001b[38;5;66;03m# Get the index of the last real token (not padding)\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/ML/nn/sprint 2/project/LLM-text-autocompletion/.venv/lib/python3.13/site-packages/torch/nn/modules/module.py:1775\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1774\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1775\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/ML/nn/sprint 2/project/LLM-text-autocompletion/.venv/lib/python3.13/site-packages/torch/nn/modules/module.py:1786\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1781\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1782\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1783\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1784\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1785\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1786\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1788\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1789\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/ML/nn/sprint 2/project/LLM-text-autocompletion/.venv/lib/python3.13/site-packages/torch/nn/modules/rnn.py:1127\u001b[39m, in \u001b[36mLSTM.forward\u001b[39m\u001b[34m(self, input, hx)\u001b[39m\n\u001b[32m   1124\u001b[39m         hx = \u001b[38;5;28mself\u001b[39m.permute_hidden(hx, sorted_indices)\n\u001b[32m   1126\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m batch_sizes \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1127\u001b[39m     result = \u001b[43m_VF\u001b[49m\u001b[43m.\u001b[49m\u001b[43mlstm\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1128\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m   1129\u001b[39m \u001b[43m        \u001b[49m\u001b[43mhx\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1130\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_flat_weights\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore[arg-type]\u001b[39;49;00m\n\u001b[32m   1131\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1132\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mnum_layers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1133\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdropout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1134\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtraining\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1135\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mbidirectional\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1136\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mbatch_first\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1137\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1138\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1139\u001b[39m     result = _VF.lstm(\n\u001b[32m   1140\u001b[39m         \u001b[38;5;28minput\u001b[39m,\n\u001b[32m   1141\u001b[39m         batch_sizes,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1148\u001b[39m         \u001b[38;5;28mself\u001b[39m.bidirectional,\n\u001b[32m   1149\u001b[39m     )\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "from train_LSTM import train_model as train_LSTM_model\n",
    "model = LSTMAutocomplete(tokenizer.vocab_size)\n",
    "train_LSTM_model(model, train_loader, val_loader, tokenizer, learning_rate=0.01, device='cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d530d6c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(train_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b60566e8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78b20486",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
